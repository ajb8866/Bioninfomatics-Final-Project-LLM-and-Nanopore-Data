{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded context: 25 rows (4219 chars).\n",
      "--- Environment: 2x A100 | Context: 8192 | Max Tokens: 4096 ---\n",
      "\n",
      "==================================================\n",
      "STARTING: DeepSeek-R1-32B\n",
      "==================================================\n",
      "**** Booting Server for: unsloth_DeepSeek-R1-Distill-Qwen-32B-GGUF_DeepSeek-R1-Distill-Qwen-32B-F16_DeepSeek-R1-Distill-Qwen-32B-F16-00001-of-00002.gguf ****\n",
      " Waiting for server... http-ok"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ready!\n",
      "ðŸš€ Benchmarking DeepSeek-R1-32B (34 remaining)...\n",
      "  - Q0 complete\n",
      "  - Q1 complete\n",
      "  - Q2 complete\n",
      "  - Q3 complete\n",
      "  - Q4 complete\n",
      "  - Q5 complete\n",
      "  - Q6 complete\n",
      "  - Q7 complete\n",
      "  - Q8 complete\n",
      "  - Q9 complete\n",
      "  - Q10 complete\n",
      "  - Q11 complete\n",
      "  - Q12 complete\n",
      "  - Q13 complete\n",
      "  - Q14 complete\n",
      "  - Q15 complete\n",
      "  - Q16 complete\n",
      "  - Q17 complete\n",
      "  - Q18 complete\n",
      "  - Q19 complete\n",
      "  - Q20 complete\n",
      "  - Q21 complete\n",
      "  - Q22 complete\n",
      "  - Q23 complete\n",
      "  - Q24 complete\n",
      "  - Q25 complete\n",
      "  - Q26 complete\n",
      "  - Q27 complete\n",
      "  - Q28 complete\n",
      "  - Q29 complete\n",
      "  - Q30 complete\n",
      "  - Q31 complete\n",
      "  - Q32 complete\n",
      "  - Q33 complete\n",
      "Shutting down DeepSeek-R1-32B...\n",
      "\n",
      "==================================================\n",
      "STARTING: Qwen3-30B\n",
      "==================================================\n",
      "**** Booting Server for: unsloth_Qwen3-30B-A3B-GGUF_BF16_Qwen3-30B-A3B-BF16-00001-of-00002.gguf ****\n",
      " Waiting for server... http-ok Ready!\n",
      "ðŸš€ Benchmarking Qwen3-30B (34 remaining)...\n",
      "  - Q0 complete\n",
      "  - Q1 complete\n",
      "  - Q2 complete\n",
      "  - Q3 complete\n",
      "  - Q4 complete\n",
      "  - Q5 complete\n",
      "  - Q6 complete\n",
      "  - Q7 complete\n",
      "  - Q8 complete\n",
      "  - Q9 complete\n",
      "  - Q10 complete\n",
      "  - Q11 complete\n",
      "  - Q12 complete\n",
      "  - Q13 complete\n",
      "  - Q14 complete\n",
      "  - Q15 complete\n",
      "  - Q16 complete\n",
      "  - Q17 complete\n",
      "  - Q18 complete\n",
      "  - Q19 complete\n",
      "  - Q20 complete\n",
      "  - Q21 complete\n",
      "  - Q22 complete\n",
      "  - Q23 complete\n",
      "  - Q24 complete\n",
      "  - Q25 complete\n",
      "  - Q26 complete\n",
      "  - Q27 complete\n",
      "  - Q28 complete\n",
      "  - Q29 complete\n",
      "  - Q30 complete\n",
      "  - Q31 complete\n",
      "  - Q32 complete\n",
      "  - Q33 complete\n",
      "Shutting down Qwen3-30B...\n",
      "\n",
      "==================================================\n",
      "STARTING: GPT-OSS-20B\n",
      "==================================================\n",
      "**** Booting Server for: ggml-org_gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf ****\n",
      " Waiting for server... http-ok Ready!\n",
      "ðŸš€ Benchmarking GPT-OSS-20B (34 remaining)...\n",
      "  - Q0 complete\n",
      "  - Q1 complete\n",
      "  - Q2 complete\n",
      "  - Q3 complete\n",
      "  - Q4 complete\n",
      "  - Q5 complete\n",
      "  - Q6 complete\n",
      "  - Q7 complete\n",
      "  - Q8 complete\n",
      "  - Q9 complete\n",
      "  - Q10 complete\n",
      "  - Q11 complete\n",
      "  - Q12 complete\n",
      "  - Q13 complete\n",
      "  - Q14 complete\n",
      "  - Q15 complete\n",
      "  - Q16 complete\n",
      "  - Q17 complete\n",
      "  - Q18 complete\n",
      "  - Q19 complete\n",
      "  - Q20 complete\n",
      "  - Q21 complete\n",
      "  - Q22 complete\n",
      "  - Q23 complete\n",
      "  - Q24 complete\n",
      "  - Q25 complete\n",
      "  - Q26 complete\n",
      "  - Q27 complete\n",
      "  - Q28 complete\n",
      "  - Q29 complete\n",
      "  - Q30 complete\n",
      "  - Q31 complete\n",
      "  - Q32 complete\n",
      "  - Q33 complete\n",
      "Shutting down GPT-OSS-20B...\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š GENERATING FINAL REPORTS\n",
      "============================================================\n",
      "âœ… Saved: benchmark_1_detailed.csv\n",
      "âœ… Saved: benchmark_2_by_model.csv\n",
      "âœ… Saved: benchmark_3_by_model_by_type.csv\n",
      "âœ… Saved: benchmark_4_global_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIG\n",
    "# ==========================================\n",
    "\n",
    "LLAMA_SERVER_PATH = \"/scratch/ajb8866/llama_cpp_src/llama.cpp/build-cuda/bin/llama-server\"\n",
    "CACHE_DIR = \"/scratch/ajb8866/cache/llama.cpp\"\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8080\n",
    "GPU_LAYERS = \"75\"\n",
    "CTX_SIZE = \"8192\"\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "MODELS = [\n",
    "    (\"DeepSeek-R1-32B\", f\"{CACHE_DIR}/unsloth_DeepSeek-R1-Distill-Qwen-32B-GGUF_DeepSeek-R1-Distill-Qwen-32B-F16_DeepSeek-R1-Distill-Qwen-32B-F16-00001-of-00002.gguf\"),\n",
    "    (\"Qwen3-30B\",       f\"{CACHE_DIR}/unsloth_Qwen3-30B-A3B-GGUF_BF16_Qwen3-30B-A3B-BF16-00001-of-00002.gguf\"),\n",
    "    (\"GPT-OSS-20B\",     f\"{CACHE_DIR}/ggml-org_gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf\")\n",
    "]\n",
    "\n",
    "TSV_FILE = \"a2_vs_d37.tsv\"\n",
    "BENCHMARK_CSV = \"a2_vs_d37_ground_truth.csv\"\n",
    "OUTPUT_DIR = \"a2_vs_d37_results_2\"\n",
    "\n",
    "# ==========================================\n",
    "# 1.  SYSTEM PROMPT \n",
    "# ==========================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert bioinformatics assistant. Your task is to answer questions based on the provided Nanocompore TSV context (first 25 rows).\n",
    "\n",
    "STRATEGY:\n",
    "1. EXTRACT: If the answer is in the data, extract it directly (no guessing).\n",
    "2. CALCULATE: If the answer requires math, compute it from the context values.\n",
    "3. INFER: Only if necessary, use limited domain knowledge; reflect uncertainty in confidence.\n",
    "\n",
    "CRITICAL OUTPUT INSTRUCTIONS:\n",
    "1. Output ONLY valid JSON.\n",
    "2. Schema (must match exactly):\n",
    "   {\n",
    "     \"answer\": \"value\",\n",
    "     \"confidence\": 0.90\n",
    "   }\n",
    "\n",
    "3. TYPE-BASED ANSWER FORMATTING RULES (match the requested Type exactly):\n",
    "   - GENOMIC_SITE: \"ref_id:pos\" (e.g., \"NC_003796.1:893\")\n",
    "   - FLOAT: digits with optional decimal/scientific notation (e.g., \"0.320240\" or \"1.2e-5\")\n",
    "   - INTEGER: digits only (e.g., \"3\")\n",
    "   - BOOLEAN: \"Yes\"/\"No\" (or \"True\"/\"False\" if the question uses that wording)\n",
    "   - STRING: short text label (avoid extra commentary)\n",
    "   - BASE: a single nucleotide letter (e.g., \"A\", \"C\", \"G\", \"T\")\n",
    "\n",
    "   - SITE_WITH_FLOAT: \"ref_id:pos (metric=value)\"\n",
    "       Example: \"NC_003796.1:893 (abs_GMM_LOR=1.078)\"\n",
    "\n",
    "   - SITE_WITH_KEYED_METRICS: \"ref_id:pos (key=value; key=value)\"\n",
    "       Example: \"NC_003796.1:893 (p=0.014041; GMM_LOR=1.078)\"\n",
    "\n",
    "   - KEYED_FLOAT_LIST: \"key=value; key=value\"\n",
    "       Example: \"sig=1.033500; nonsig=0.492000\"\n",
    "\n",
    "   - KEYED_MIXED_LIST: \"key=value; key=value; key=value\"\n",
    "       Values may be int/float/string, but always keep \"key=value\".\n",
    "       Example: \"effect=0.978; coverage=-0.181; stronger=effect_size\"\n",
    "\n",
    "   - RANKING_WITH_SCORES: semicolon-separated ordered items:\n",
    "       \"site(value); site(value); ...\"\n",
    "       Example: \"NC_003796.1:893(0.274799); NC_003796.1:870(0.258363)\"\n",
    "\n",
    "   - RUN_AND_TREND: \"start-end; trend=VALUE\"\n",
    "       Example: \"882-886; trend=mixed\"\n",
    "\n",
    "   - COUNT_AND_SITE_LIST: \"count=N; sites=site1; site2; site3\"\n",
    "       Example: \"count=3; sites=NC_003796.1:893; NC_003796.1:891; NC_003796.1:870\"\n",
    "\n",
    "4. CATEGORY SET (use these meanings when reasoning; do NOT output category labels):\n",
    "   - Directionality & Change: effect direction/sign (e.g., GMM_LOR, positive/negative fractions, run trends)\n",
    "   - Derived Metrics: computed from counts (e.g., mod fraction, deltas, rankings of derived values)\n",
    "   - Model vs Counts Consistency: compare model outputs (GMM_LOR) to ratios derived from raw counts\n",
    "   - Statistical Metrics & Significance: p-values/q-values, correlations with -log10(p), significance behavior\n",
    "   - High-Confidence Predictions: thresholding rules, candidate lists, significant+large-effect filters\n",
    "   - Site/Feature Annotation: ref_kmer, base/k-mer properties, motif-like summaries\n",
    "   - Coverage & Depth: coverage/total reads, coverage imbalance, median-based coverage filters\n",
    "\n",
    "5. CONFIDENCE:\n",
    "Use ONLY one of these values: 1.00, 0.90, 0.75, 0.55, 0.30\n",
    "\n",
    "- 1.00 = directly copied from a single unambiguous row in CONTEXT (unique match)\n",
    "- 0.90 = simple calculation from context values with no ambiguity\n",
    "- 0.75 = multi-step calculation or multiple rows but still clear\n",
    "- 0.55 = requires inference/assumption or context is incomplete/ambiguous\n",
    "- 0.30 = best guess; answer not fully supported by context\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. INFERENCE & SERVER UTILS\n",
    "# ==========================================\n",
    "\n",
    "def load_context_chunk(file_path, rows=25):\n",
    "    try:\n",
    "        sep = '\\t' if file_path.endswith('.tsv') else ','\n",
    "        df = pd.read_csv(file_path, sep=sep)\n",
    "        chunk = df.head(rows)\n",
    "        context_str = chunk.to_csv(index=False, sep='\\t')\n",
    "        print(f\"Loaded context: {len(chunk)} rows ({len(context_str)} chars).\")\n",
    "        return context_str\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading context file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_model_id(client, fallback=\"default\"):\n",
    "    \"\"\"\n",
    "    Try to use the actual model id if exposed; fallback to 'default'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        if hasattr(models, \"data\") and models.data:\n",
    "            return models.data[0].id\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fallback\n",
    "\n",
    "\n",
    "def wait_for_chat_ready(client, model_id=\"default\", timeout_s=1500):\n",
    "    \"\"\"\n",
    "    Real readiness check: server must serve a tiny chat completion.\n",
    "    /v1/models may respond while the model is still loading.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    last_err = None\n",
    "    while time.time() - t0 < timeout_s:\n",
    "        try:\n",
    "            client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=1,\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            msg = str(e)\n",
    "            if \"Loading model\" in msg or \"503\" in msg:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "    print(f\"\\nChat readiness timed out. Last error: {last_err}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def chat_with_retries(client, *, model, messages, temperature, max_tokens, max_attempts=25):\n",
    "    \"\"\"\n",
    "    Retry transient 503 / 'Loading model' errors during warmup.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            return client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            msg = str(e)\n",
    "            if \"Loading model\" in msg or \"503\" in msg:\n",
    "                time.sleep(min(2 * attempt, 20))\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(f\"Persistent 503/Loading model after retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "# Load once\n",
    "CONTEXT_TEXT = load_context_chunk(TSV_FILE, rows=25)\n",
    "QUESTIONS_DF = pd.read_csv(BENCHMARK_CSV)\n",
    "\n",
    "# Ensure question_id exists\n",
    "if 'question_id' not in QUESTIONS_DF.columns:\n",
    "    QUESTIONS_DF.insert(0, 'question_id', range(len(QUESTIONS_DF)))\n",
    "\n",
    "\n",
    "def start_server(model_path, model_name=None):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Critical Error: Model file not found at:\\n{model_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"**** Booting Server for: {os.path.basename(model_path)} ****\")\n",
    "\n",
    "    # Log stderr to file for debugging load failures\n",
    "    stderr_target = subprocess.PIPE\n",
    "    log_fh = None\n",
    "    if model_name:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        log_path = os.path.join(OUTPUT_DIR, f\"{model_name}_server.stderr.log\")\n",
    "        log_fh = open(log_path, \"wb\")\n",
    "        stderr_target = log_fh\n",
    "\n",
    "    cmd = [\n",
    "        LLAMA_SERVER_PATH, \"-m\", model_path,\n",
    "        \"--host\", HOST, \"--port\", str(PORT),\n",
    "        \"--ctx-size\", str(CTX_SIZE),\n",
    "        \"--n-gpu-layers\", str(GPU_LAYERS),\n",
    "        \"--tensor-split\", \"50,50\",\n",
    "        \"--parallel\", \"1\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=stderr_target)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to launch process: {e}\")\n",
    "        if log_fh:\n",
    "            log_fh.close()\n",
    "        return None\n",
    "\n",
    "    print(\" Waiting for server...\", end=\"\", flush=True)\n",
    "    client = OpenAI(base_url=f\"http://{HOST}:{PORT}/v1\", api_key=\"x\")\n",
    "\n",
    "    # Phase 1: wait for HTTP layer\n",
    "    for _ in range(180):\n",
    "        try:\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "            client.models.list()\n",
    "            print(\" http-ok\", end=\"\", flush=True)\n",
    "            break\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    if process.poll() is not None:\n",
    "        print(\"\\nServer process exited early (check stderr log).\")\n",
    "        try:\n",
    "            process.kill()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if log_fh:\n",
    "            log_fh.close()\n",
    "        return None\n",
    "\n",
    "    # Use actual model id if exposed\n",
    "    model_id = extract_model_id(client, fallback=\"default\")\n",
    "\n",
    "    # Phase 2: wait for chat readiness\n",
    "    ok = wait_for_chat_ready(client, model_id=model_id, timeout_s=900)\n",
    "    if ok:\n",
    "        print(\" Ready!\")\n",
    "        return process\n",
    "\n",
    "    print(\"\\nServer timed out waiting for model readiness.\")\n",
    "    try:\n",
    "        process.kill()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if log_fh:\n",
    "        log_fh.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_benchmark(model_name):\n",
    "    client = OpenAI(base_url=f\"http://{HOST}:{PORT}/v1\", api_key=\"na\")\n",
    "    model_id = extract_model_id(client, fallback=\"default\")\n",
    "\n",
    "    save_dir = os.path.join(OUTPUT_DIR, model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    output_file = os.path.join(save_dir, \"results.jsonl\")\n",
    "\n",
    "    done_ids = set()\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    done_ids.add(json.loads(line)['question_id'])\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    remaining_count = len(QUESTIONS_DF) - len(done_ids)\n",
    "    print(f\"ðŸš€ Benchmarking {model_name} ({remaining_count} remaining)...\")\n",
    "\n",
    "    if remaining_count == 0:\n",
    "        return\n",
    "\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        for idx, row in QUESTIONS_DF.iterrows():\n",
    "            q_id = int(row['question_id'])\n",
    "            if q_id in done_ids:\n",
    "                continue\n",
    "\n",
    "            prompt = f\"CONTEXT DATA:\\n{CONTEXT_TEXT}\\n\\nQUESTION:\\n{row['Question']}\"\n",
    "\n",
    "            try:\n",
    "                response = chat_with_retries(\n",
    "                    client,\n",
    "                    model=model_id,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=MAX_TOKENS\n",
    "                )\n",
    "                raw_text = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error Q{q_id}: {e}\")\n",
    "                raw_text = \"ERROR_API\"\n",
    "\n",
    "            result = {\n",
    "                \"model\": model_name,\n",
    "                \"question_id\": q_id,\n",
    "                \"question\": row['Question'],\n",
    "                \"llm_response\": raw_text\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "            f_out.flush()\n",
    "            print(f\"  - Q{q_id} complete\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. EVALUATION METRICS LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def extract_first_json_object(s: str) -> str | None:\n",
    "    # Prefer fenced JSON if present anywhere\n",
    "    m = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", s, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # Otherwise, brace-balance from first '{'\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "\n",
    "        # track strings so braces inside strings don't count\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "                continue\n",
    "\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return s[start:i+1].strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def validate_and_parse_response(response_str):\n",
    "    if pd.isna(response_str):\n",
    "        return False, None, None\n",
    "\n",
    "    s = str(response_str).strip()\n",
    "\n",
    "    # 1) Fast path: already pure JSON\n",
    "    try:\n",
    "        data = json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        # 2) Try extracting JSON from inside mixed text\n",
    "        candidate = extract_first_json_object(s)\n",
    "        if candidate is None:\n",
    "            return False, None, None\n",
    "        try:\n",
    "            data = json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            return False, None, None\n",
    "\n",
    "    # Validate schema\n",
    "    if not (isinstance(data, dict) and \"answer\" in data and \"confidence\" in data):\n",
    "        return False, None, None\n",
    "\n",
    "    raw_ans = data[\"answer\"]\n",
    "    raw_conf = data[\"confidence\"]\n",
    "\n",
    "    # normalize answer to string (helps later comparisons)\n",
    "    ans = None if raw_ans is None else str(raw_ans)\n",
    "\n",
    "    # confidence must be a real number in [0,1]\n",
    "    try:\n",
    "        conf = float(raw_conf)\n",
    "    except (ValueError, TypeError):\n",
    "        return False, ans, None\n",
    "\n",
    "    if 0.0 <= conf <= 1.0:\n",
    "        return True, ans, conf\n",
    "    return False, ans, None\n",
    "\n",
    "\n",
    "def check_exactness(row):\n",
    "    \"\"\"\n",
    "    Checker that supports the NEW Types used in your benchmark CSV.\n",
    "    \"\"\"\n",
    "    if pd.isna(row['parsed_answer']) or row['parsed_answer'] is None:\n",
    "        return False\n",
    "\n",
    "    gt = str(row['Answer']).strip()\n",
    "    pred = str(row['parsed_answer']).strip()\n",
    "    dtype = str(row['Type']).strip()\n",
    "\n",
    "    gt_l = gt.lower().replace(\" \", \"\")\n",
    "    pred_l = pred.lower().replace(\" \", \"\")\n",
    "\n",
    "    try:\n",
    "        if dtype == 'FLOAT':\n",
    "            return np.isclose(float(gt), float(pred), rtol=1e-3)\n",
    "\n",
    "        if dtype == 'INTEGER':\n",
    "            return int(float(gt)) == int(float(pred))\n",
    "\n",
    "        if dtype == 'BOOLEAN':\n",
    "            true_vals = {'true', 'yes', '1', 't', 'correct'}\n",
    "            false_vals = {'false', 'no', '0', 'f', 'incorrect'}\n",
    "            if pred_l not in true_vals and pred_l not in false_vals:\n",
    "                return False\n",
    "            return (gt_l in true_vals) == (pred_l in true_vals)\n",
    "\n",
    "        if dtype == 'BASE':\n",
    "            return gt.strip().upper() == pred.strip().upper()\n",
    "\n",
    "        if dtype == 'GENOMIC_SITE':\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        if dtype in {'STRING', 'RUN_AND_TREND', 'KEYED_MIXED_LIST'}:\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        if dtype == 'KEYED_FLOAT_LIST':\n",
    "            # compare as unordered key->float maps\n",
    "            def parse_keyed_floats(s):\n",
    "                out = {}\n",
    "                for part in s.split(';'):\n",
    "                    part = part.strip()\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    k, v = part.split('=', 1)\n",
    "                    out[k.strip()] = float(v.strip())\n",
    "                return out\n",
    "\n",
    "            a = parse_keyed_floats(gt)\n",
    "            b = parse_keyed_floats(pred)\n",
    "            if a.keys() != b.keys():\n",
    "                return False\n",
    "            return all(np.isclose(a[k], b[k], rtol=1e-3) for k in a)\n",
    "\n",
    "        if dtype == 'COUNT_AND_SITE_LIST':\n",
    "            # normalize list ordering after \"sites=\"\n",
    "            def parse_count_sites(s):\n",
    "                parts = [p.strip() for p in s.split(';') if p.strip()]\n",
    "                first = parts[0] if parts else \"\"\n",
    "                m = re.search(r\"count\\s*=\\s*(\\d+)\", first, flags=re.I)\n",
    "                count = int(m.group(1)) if m else None\n",
    "\n",
    "                sites = []\n",
    "                m2 = re.search(r\"sites\\s*=\\s*(.*)\", first, flags=re.I)\n",
    "                if m2 and m2.group(1).strip():\n",
    "                    sites.append(m2.group(1).strip())\n",
    "                sites.extend(parts[1:])\n",
    "                sites = [x.strip() for x in sites if x.strip()]\n",
    "                return count, sorted(sites)\n",
    "\n",
    "            c1, s1 = parse_count_sites(gt)\n",
    "            c2, s2 = parse_count_sites(pred)\n",
    "            return (c1 == c2) and (s1 == s2)\n",
    "\n",
    "        if dtype == 'RANKING_WITH_SCORES':\n",
    "            # require exact order; normalize whitespace\n",
    "            def norm_rank(s):\n",
    "                return \";\".join([p.strip().replace(\" \", \"\") for p in s.split(\";\") if p.strip()])\n",
    "            return norm_rank(gt).lower() == norm_rank(pred).lower()\n",
    "\n",
    "        if dtype in {'SITE_WITH_FLOAT', 'SITE_WITH_KEYED_METRICS'}:\n",
    "            # structured: compare normalized strings\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        # fallback\n",
    "        return gt_l == pred_l\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def calculate_calibration(df):\n",
    "    \"\"\"\n",
    "    Brier Score: mean squared error between predicted confidence and correctness.\n",
    "    Lower is better; 0.0 is perfect.\n",
    "    \"\"\"\n",
    "    valid_df = df[df['parsed_confidence'].notna()].copy()\n",
    "    if len(valid_df) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    y = valid_df['is_correct'].astype(int).to_numpy()\n",
    "    p = valid_df['parsed_confidence'].astype(float).to_numpy()\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "\n",
    "    brier = np.mean((p - y) ** 2)\n",
    "    return float(brier)\n",
    "\n",
    "\n",
    "def calculate_auc(df):\n",
    "    \"\"\"\n",
    "    ROC AUC of confidence as a score for correctness (binary label).\n",
    "    Rank-based formula (Mannâ€“Whitney), handles ties, no sklearn dependency.\n",
    "    \"\"\"\n",
    "    valid_df = df[df['parsed_confidence'].notna()].copy()\n",
    "\n",
    "    # AUC undefined if only one class\n",
    "    if len(valid_df) < 2 or valid_df['is_correct'].nunique() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    y_true = valid_df['is_correct'].astype(int).to_numpy()\n",
    "    y_score = valid_df['parsed_confidence'].astype(float).to_numpy()\n",
    "\n",
    "    n_pos = int(y_true.sum())\n",
    "    n_neg = int(len(y_true) - n_pos)\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return np.nan\n",
    "\n",
    "    ranks = pd.Series(y_score).rank(method=\"average\").to_numpy()\n",
    "    sum_ranks_pos = ranks[y_true == 1].sum()\n",
    "\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "# ==========================================\n",
    "# 4. FINAL REPORT GENERATION (THE 3 CSVs)\n",
    "# ==========================================\n",
    "\n",
    "def generate_final_report():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š GENERATING FINAL REPORTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    all_data = []\n",
    "    model_summaries = []\n",
    "\n",
    "    for model_name, _ in MODELS:\n",
    "        result_file = os.path.join(OUTPUT_DIR, model_name, \"results.jsonl\")\n",
    "        if not os.path.exists(result_file):\n",
    "            print(f\"No results found for {model_name}\")\n",
    "            continue\n",
    "\n",
    "        model_results = pd.read_json(result_file, lines=True)\n",
    "        model_results['question_id'] = model_results['question_id'].astype(int)\n",
    "\n",
    "        # Merge using question_id\n",
    "        df = pd.merge(QUESTIONS_DF, model_results, on='question_id', how='inner')\n",
    "        df['Model'] = model_name\n",
    "\n",
    "        # Apply Scoring\n",
    "        parsed = df['llm_response'].apply(validate_and_parse_response)\n",
    "        df['format_valid'] = parsed.apply(lambda x: x[0])\n",
    "        df['parsed_answer'] = parsed.apply(lambda x: x[1])\n",
    "        df['parsed_confidence'] = parsed.apply(lambda x: x[2])\n",
    "        df['is_correct'] = df.apply(check_exactness, axis=1)\n",
    "\n",
    "        all_data.append(df)\n",
    "\n",
    "        # Stats\n",
    "        acc = float(df['is_correct'].mean())\n",
    "        fmt = float(df['format_valid'].mean())\n",
    "        brier = calculate_calibration(df)\n",
    "        auc = calculate_auc(df)\n",
    "\n",
    "        model_summaries.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Format_Adherence\": fmt,\n",
    "            \"Brier (Calibration)\": brier,\n",
    "            \"AUC\": auc\n",
    "        })\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data found to report on.\")\n",
    "        return\n",
    "\n",
    "    # CSV 1: DETAILED BY QUESTION\n",
    "    big_df = pd.concat(all_data, ignore_index=True)\n",
    "    cols = [\n",
    "        'Model', 'question_id', 'Question', 'Type', 'Category',\n",
    "        'Answer', 'parsed_answer', 'parsed_confidence',\n",
    "        'is_correct', 'format_valid', 'llm_response'\n",
    "    ]\n",
    "    safe_cols = [c for c in cols if c in big_df.columns]\n",
    "    big_df[safe_cols].to_csv(\"benchmark_1_detailed.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_1_detailed.csv\")\n",
    "\n",
    "    # CSV 2: SUMMARY BY MODEL\n",
    "    summary_df = pd.DataFrame(model_summaries)\n",
    "    summary_df.to_csv(\"benchmark_2_by_model.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_2_by_model.csv\")\n",
    "\n",
    "        # CSV 3: SUMMARY BY MODEL x TYPE (and Category)\n",
    "    by_model_by_type = (\n",
    "        big_df\n",
    "        .groupby(['Model', 'Type', 'Category'], as_index=False)\n",
    "        .agg(\n",
    "            N=('question_id', 'count'),\n",
    "            Accuracy=('is_correct', 'mean'),\n",
    "            Format_Adherence=('format_valid', 'mean'),\n",
    "            Mean_Confidence=('parsed_confidence', 'mean'),\n",
    "        )\n",
    "    )\n",
    "    by_model_by_type.to_csv(\"benchmark_3_by_model_by_type.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_3_by_model_by_type.csv\")\n",
    "\n",
    "    # CSV 4: GLOBAL STATS BY TYPE (and Category) across all models\n",
    "    global_stats = (\n",
    "        big_df\n",
    "        .groupby(['Type', 'Category'], as_index=False)\n",
    "        .agg(\n",
    "            N=('question_id', 'count'),\n",
    "            Accuracy=('is_correct', 'mean'),\n",
    "            Format_Adherence=('format_valid', 'mean'),\n",
    "            Mean_Confidence=('parsed_confidence', 'mean'),\n",
    "        )\n",
    "    )\n",
    "    global_stats.to_csv(\"benchmark_4_global_stats.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_4_global_stats.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION FLOW\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Environment: 2x A100 | Context: {CTX_SIZE} | Max Tokens: {MAX_TOKENS} ---\")\n",
    "\n",
    "    for friendly_name, model_path in MODELS:\n",
    "        print(f\"\\n{'=' * 50}\\nSTARTING: {friendly_name}\\n{'=' * 50}\")\n",
    "\n",
    "        server_proc = start_server(model_path, model_name=friendly_name)\n",
    "        if server_proc is None:\n",
    "            print(f\"Skipping {friendly_name} due to server failure.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            run_benchmark(friendly_name)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user.\")\n",
    "            if server_proc:\n",
    "                server_proc.kill()\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error: {e}\")\n",
    "        finally:\n",
    "            print(f\"Shutting down {friendly_name}...\")\n",
    "            if server_proc:\n",
    "                server_proc.terminate()\n",
    "                try:\n",
    "                    server_proc.wait(timeout=5)\n",
    "                except Exception:\n",
    "                    server_proc.kill()\n",
    "            time.sleep(3)\n",
    "\n",
    "    generate_final_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIG\n",
    "# ==========================================\n",
    "\n",
    "LLAMA_SERVER_PATH = \"/scratch/ajb8866/llama_cpp_src/llama.cpp/build-cuda/bin/llama-server\"\n",
    "CACHE_DIR = \"/scratch/ajb8866/cache/llama.cpp\"\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8080\n",
    "GPU_LAYERS = \"75\"\n",
    "CTX_SIZE = \"8192\"\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "MODELS = [\n",
    "    (\"DeepSeek-R1-32B\", f\"{CACHE_DIR}/unsloth_DeepSeek-R1-Distill-Qwen-32B-GGUF_DeepSeek-R1-Distill-Qwen-32B-F16_DeepSeek-R1-Distill-Qwen-32B-F16-00001-of-00002.gguf\"),\n",
    "    (\"Qwen3-30B\",       f\"{CACHE_DIR}/unsloth_Qwen3-30B-A3B-GGUF_BF16_Qwen3-30B-A3B-BF16-00001-of-00002.gguf\"),\n",
    "    (\"GPT-OSS-20B\",     f\"{CACHE_DIR}/ggml-org_gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf\")\n",
    "]\n",
    "\n",
    "TSV_FILE = \"a2_vs_d37.tsv\"\n",
    "BENCHMARK_CSV = \"a2_vs_d37_ground_truth.csv\"\n",
    "OUTPUT_DIR = \"a2_vs_d37_results_2\"\n",
    "\n",
    "# ==========================================\n",
    "# 1.  SYSTEM PROMPT \n",
    "# ==========================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert bioinformatics assistant. Your task is to answer questions based on the provided Nanocompore TSV context (first 25 rows).\n",
    "\n",
    "STRATEGY:\n",
    "1. EXTRACT: If the answer is in the data, extract it directly (no guessing).\n",
    "2. CALCULATE: If the answer requires math, compute it from the context values.\n",
    "3. INFER: Only if necessary, use limited domain knowledge; reflect uncertainty in confidence.\n",
    "\n",
    "CRITICAL OUTPUT INSTRUCTIONS:\n",
    "1. Output ONLY valid JSON.\n",
    "2. Schema (must match exactly):\n",
    "   {\n",
    "     \"answer\": \"value\",\n",
    "     \"confidence\": 0.90\n",
    "   }\n",
    "\n",
    "3. TYPE-BASED ANSWER FORMATTING RULES (match the requested Type exactly):\n",
    "   - GENOMIC_SITE: \"ref_id:pos\" (e.g., \"NC_003796.1:893\")\n",
    "   - FLOAT: digits with optional decimal/scientific notation (e.g., \"0.320240\" or \"1.2e-5\")\n",
    "   - INTEGER: digits only (e.g., \"3\")\n",
    "   - BOOLEAN: \"Yes\"/\"No\" (or \"True\"/\"False\" if the question uses that wording)\n",
    "   - STRING: short text label (avoid extra commentary)\n",
    "   - BASE: a single nucleotide letter (e.g., \"A\", \"C\", \"G\", \"T\")\n",
    "\n",
    "   - SITE_WITH_FLOAT: \"ref_id:pos (metric=value)\"\n",
    "       Example: \"NC_003796.1:893 (abs_GMM_LOR=1.078)\"\n",
    "\n",
    "   - SITE_WITH_KEYED_METRICS: \"ref_id:pos (key=value; key=value)\"\n",
    "       Example: \"NC_003796.1:893 (p=0.014041; GMM_LOR=1.078)\"\n",
    "\n",
    "   - KEYED_FLOAT_LIST: \"key=value; key=value\"\n",
    "       Example: \"sig=1.033500; nonsig=0.492000\"\n",
    "\n",
    "   - KEYED_MIXED_LIST: \"key=value; key=value; key=value\"\n",
    "       Values may be int/float/string, but always keep \"key=value\".\n",
    "       Example: \"effect=0.978; coverage=-0.181; stronger=effect_size\"\n",
    "\n",
    "   - RANKING_WITH_SCORES: semicolon-separated ordered items:\n",
    "       \"site(value); site(value); ...\"\n",
    "       Example: \"NC_003796.1:893(0.274799); NC_003796.1:870(0.258363)\"\n",
    "\n",
    "   - RUN_AND_TREND: \"start-end; trend=VALUE\"\n",
    "       Example: \"882-886; trend=mixed\"\n",
    "\n",
    "   - COUNT_AND_SITE_LIST: \"count=N; sites=site1; site2; site3\"\n",
    "       Example: \"count=3; sites=NC_003796.1:893; NC_003796.1:891; NC_003796.1:870\"\n",
    "\n",
    "4. CATEGORY SET (use these meanings when reasoning; do NOT output category labels):\n",
    "   - Directionality & Change: effect direction/sign (e.g., GMM_LOR, positive/negative fractions, run trends)\n",
    "   - Derived Metrics: computed from counts (e.g., mod fraction, deltas, rankings of derived values)\n",
    "   - Model vs Counts Consistency: compare model outputs (GMM_LOR) to ratios derived from raw counts\n",
    "   - Statistical Metrics & Significance: p-values/q-values, correlations with -log10(p), significance behavior\n",
    "   - High-Confidence Predictions: thresholding rules, candidate lists, significant+large-effect filters\n",
    "   - Site/Feature Annotation: ref_kmer, base/k-mer properties, motif-like summaries\n",
    "   - Coverage & Depth: coverage/total reads, coverage imbalance, median-based coverage filters\n",
    "\n",
    "5. CONFIDENCE:\n",
    "Use ONLY one of these values: 1.00, 0.90, 0.75, 0.55, 0.30\n",
    "\n",
    "- 1.00 = directly copied from a single unambiguous row in CONTEXT (unique match)\n",
    "- 0.90 = simple calculation from context values with no ambiguity\n",
    "- 0.75 = multi-step calculation or multiple rows but still clear\n",
    "- 0.55 = requires inference/assumption or context is incomplete/ambiguous\n",
    "- 0.30 = best guess; answer not fully supported by context\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. INFERENCE & SERVER UTILS\n",
    "# ==========================================\n",
    "\n",
    "def load_context_chunk(file_path, rows=25):\n",
    "    try:\n",
    "        sep = '\\t' if file_path.endswith('.tsv') else ','\n",
    "        df = pd.read_csv(file_path, sep=sep)\n",
    "        chunk = df.head(rows)\n",
    "        context_str = chunk.to_csv(index=False, sep='\\t')\n",
    "        print(f\"Loaded context: {len(chunk)} rows ({len(context_str)} chars).\")\n",
    "        return context_str\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading context file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_model_id(client, fallback=\"default\"):\n",
    "    \"\"\"\n",
    "    Try to use the actual model id if exposed; fallback to 'default'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        if hasattr(models, \"data\") and models.data:\n",
    "            return models.data[0].id\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fallback\n",
    "\n",
    "\n",
    "def wait_for_chat_ready(client, model_id=\"default\", timeout_s=1500):\n",
    "    \"\"\"\n",
    "    Real readiness check: server must serve a tiny chat completion.\n",
    "    /v1/models may respond while the model is still loading.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    last_err = None\n",
    "    while time.time() - t0 < timeout_s:\n",
    "        try:\n",
    "            client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=1,\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            msg = str(e)\n",
    "            if \"Loading model\" in msg or \"503\" in msg:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "    print(f\"\\nChat readiness timed out. Last error: {last_err}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def chat_with_retries(client, *, model, messages, temperature, max_tokens, max_attempts=25):\n",
    "    \"\"\"\n",
    "    Retry transient 503 / 'Loading model' errors during warmup.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            return client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            msg = str(e)\n",
    "            if \"Loading model\" in msg or \"503\" in msg:\n",
    "                time.sleep(min(2 * attempt, 20))\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(f\"Persistent 503/Loading model after retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "# Load once\n",
    "CONTEXT_TEXT = load_context_chunk(TSV_FILE, rows=25)\n",
    "QUESTIONS_DF = pd.read_csv(BENCHMARK_CSV)\n",
    "\n",
    "# Ensure question_id exists\n",
    "if 'question_id' not in QUESTIONS_DF.columns:\n",
    "    QUESTIONS_DF.insert(0, 'question_id', range(len(QUESTIONS_DF)))\n",
    "\n",
    "\n",
    "def start_server(model_path, model_name=None):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Critical Error: Model file not found at:\\n{model_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"**** Booting Server for: {os.path.basename(model_path)} ****\")\n",
    "\n",
    "    # Log stderr to file for debugging load failures\n",
    "    stderr_target = subprocess.PIPE\n",
    "    log_fh = None\n",
    "    if model_name:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        log_path = os.path.join(OUTPUT_DIR, f\"{model_name}_server.stderr.log\")\n",
    "        log_fh = open(log_path, \"wb\")\n",
    "        stderr_target = log_fh\n",
    "\n",
    "    cmd = [\n",
    "        LLAMA_SERVER_PATH, \"-m\", model_path,\n",
    "        \"--host\", HOST, \"--port\", str(PORT),\n",
    "        \"--ctx-size\", str(CTX_SIZE),\n",
    "        \"--n-gpu-layers\", str(GPU_LAYERS),\n",
    "        \"--tensor-split\", \"50,50\",\n",
    "        \"--parallel\", \"1\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=stderr_target)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to launch process: {e}\")\n",
    "        if log_fh:\n",
    "            log_fh.close()\n",
    "        return None\n",
    "\n",
    "    print(\" Waiting for server...\", end=\"\", flush=True)\n",
    "    client = OpenAI(base_url=f\"http://{HOST}:{PORT}/v1\", api_key=\"x\")\n",
    "\n",
    "    # Phase 1: wait for HTTP layer\n",
    "    for _ in range(180):\n",
    "        try:\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "            client.models.list()\n",
    "            print(\" http-ok\", end=\"\", flush=True)\n",
    "            break\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    if process.poll() is not None:\n",
    "        print(\"\\nServer process exited early (check stderr log).\")\n",
    "        try:\n",
    "            process.kill()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if log_fh:\n",
    "            log_fh.close()\n",
    "        return None\n",
    "\n",
    "    # Use actual model id if exposed\n",
    "    model_id = extract_model_id(client, fallback=\"default\")\n",
    "\n",
    "    # Phase 2: wait for chat readiness\n",
    "    ok = wait_for_chat_ready(client, model_id=model_id, timeout_s=900)\n",
    "    if ok:\n",
    "        print(\" Ready!\")\n",
    "        return process\n",
    "\n",
    "    print(\"\\nServer timed out waiting for model readiness.\")\n",
    "    try:\n",
    "        process.kill()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if log_fh:\n",
    "        log_fh.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_benchmark(model_name):\n",
    "    client = OpenAI(base_url=f\"http://{HOST}:{PORT}/v1\", api_key=\"na\")\n",
    "    model_id = extract_model_id(client, fallback=\"default\")\n",
    "\n",
    "    save_dir = os.path.join(OUTPUT_DIR, model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    output_file = os.path.join(save_dir, \"results.jsonl\")\n",
    "\n",
    "    done_ids = set()\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    done_ids.add(json.loads(line)['question_id'])\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    remaining_count = len(QUESTIONS_DF) - len(done_ids)\n",
    "    print(f\"ðŸš€ Benchmarking {model_name} ({remaining_count} remaining)...\")\n",
    "\n",
    "    if remaining_count == 0:\n",
    "        return\n",
    "\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        for idx, row in QUESTIONS_DF.iterrows():\n",
    "            q_id = int(row['question_id'])\n",
    "            if q_id in done_ids:\n",
    "                continue\n",
    "\n",
    "            prompt = f\"CONTEXT DATA:\\n{CONTEXT_TEXT}\\n\\nQUESTION:\\n{row['Question']}\"\n",
    "\n",
    "            try:\n",
    "                response = chat_with_retries(\n",
    "                    client,\n",
    "                    model=model_id,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=MAX_TOKENS\n",
    "                )\n",
    "                raw_text = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error Q{q_id}: {e}\")\n",
    "                raw_text = \"ERROR_API\"\n",
    "\n",
    "            result = {\n",
    "                \"model\": model_name,\n",
    "                \"question_id\": q_id,\n",
    "                \"question\": row['Question'],\n",
    "                \"llm_response\": raw_text\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "            f_out.flush()\n",
    "            print(f\"  - Q{q_id} complete\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. EVALUATION METRICS LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def extract_first_json_object(s: str) -> str | None:\n",
    "    # Prefer fenced JSON if present anywhere\n",
    "    m = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", s, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # Otherwise, brace-balance from first '{'\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "\n",
    "        # track strings so braces inside strings don't count\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "                continue\n",
    "\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return s[start:i+1].strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def validate_and_parse_response(response_str):\n",
    "    if pd.isna(response_str):\n",
    "        return False, None, None\n",
    "\n",
    "    s = str(response_str).strip()\n",
    "\n",
    "    # 1) Fast path: already pure JSON\n",
    "    try:\n",
    "        data = json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        # 2) Try extracting JSON from inside mixed text\n",
    "        candidate = extract_first_json_object(s)\n",
    "        if candidate is None:\n",
    "            return False, None, None\n",
    "        try:\n",
    "            data = json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            return False, None, None\n",
    "\n",
    "    # Validate schema\n",
    "    if not (isinstance(data, dict) and \"answer\" in data and \"confidence\" in data):\n",
    "        return False, None, None\n",
    "\n",
    "    raw_ans = data[\"answer\"]\n",
    "    raw_conf = data[\"confidence\"]\n",
    "\n",
    "    # normalize answer to string (helps later comparisons)\n",
    "    ans = None if raw_ans is None else str(raw_ans)\n",
    "\n",
    "    # confidence must be a real number in [0,1]\n",
    "    try:\n",
    "        conf = float(raw_conf)\n",
    "    except (ValueError, TypeError):\n",
    "        return False, ans, None\n",
    "\n",
    "    if 0.0 <= conf <= 1.0:\n",
    "        return True, ans, conf\n",
    "    return False, ans, None\n",
    "\n",
    "\n",
    "def check_exactness(row):\n",
    "    \"\"\"\n",
    "    Checker that supports the NEW Types used in your benchmark CSV.\n",
    "    \"\"\"\n",
    "    if pd.isna(row['parsed_answer']) or row['parsed_answer'] is None:\n",
    "        return False\n",
    "\n",
    "    gt = str(row['Answer']).strip()\n",
    "    pred = str(row['parsed_answer']).strip()\n",
    "    dtype = str(row['Type']).strip()\n",
    "\n",
    "    gt_l = gt.lower().replace(\" \", \"\")\n",
    "    pred_l = pred.lower().replace(\" \", \"\")\n",
    "\n",
    "    try:\n",
    "        if dtype == 'FLOAT':\n",
    "            return np.isclose(float(gt), float(pred), rtol=1e-3)\n",
    "\n",
    "        if dtype == 'INTEGER':\n",
    "            return int(float(gt)) == int(float(pred))\n",
    "\n",
    "        if dtype == 'BOOLEAN':\n",
    "            true_vals = {'true', 'yes', '1', 't', 'correct'}\n",
    "            false_vals = {'false', 'no', '0', 'f', 'incorrect'}\n",
    "            if pred_l not in true_vals and pred_l not in false_vals:\n",
    "                return False\n",
    "            return (gt_l in true_vals) == (pred_l in true_vals)\n",
    "\n",
    "        if dtype == 'BASE':\n",
    "            return gt.strip().upper() == pred.strip().upper()\n",
    "\n",
    "        if dtype == 'GENOMIC_SITE':\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        if dtype in {'STRING', 'RUN_AND_TREND', 'KEYED_MIXED_LIST'}:\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        if dtype == 'KEYED_FLOAT_LIST':\n",
    "            # compare as unordered key->float maps\n",
    "            def parse_keyed_floats(s):\n",
    "                out = {}\n",
    "                for part in s.split(';'):\n",
    "                    part = part.strip()\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    k, v = part.split('=', 1)\n",
    "                    out[k.strip()] = float(v.strip())\n",
    "                return out\n",
    "\n",
    "            a = parse_keyed_floats(gt)\n",
    "            b = parse_keyed_floats(pred)\n",
    "            if a.keys() != b.keys():\n",
    "                return False\n",
    "            return all(np.isclose(a[k], b[k], rtol=1e-3) for k in a)\n",
    "\n",
    "        if dtype == 'COUNT_AND_SITE_LIST':\n",
    "            # normalize list ordering after \"sites=\"\n",
    "            def parse_count_sites(s):\n",
    "                parts = [p.strip() for p in s.split(';') if p.strip()]\n",
    "                first = parts[0] if parts else \"\"\n",
    "                m = re.search(r\"count\\s*=\\s*(\\d+)\", first, flags=re.I)\n",
    "                count = int(m.group(1)) if m else None\n",
    "\n",
    "                sites = []\n",
    "                m2 = re.search(r\"sites\\s*=\\s*(.*)\", first, flags=re.I)\n",
    "                if m2 and m2.group(1).strip():\n",
    "                    sites.append(m2.group(1).strip())\n",
    "                sites.extend(parts[1:])\n",
    "                sites = [x.strip() for x in sites if x.strip()]\n",
    "                return count, sorted(sites)\n",
    "\n",
    "            c1, s1 = parse_count_sites(gt)\n",
    "            c2, s2 = parse_count_sites(pred)\n",
    "            return (c1 == c2) and (s1 == s2)\n",
    "\n",
    "        if dtype == 'RANKING_WITH_SCORES':\n",
    "            # require exact order; normalize whitespace\n",
    "            def norm_rank(s):\n",
    "                return \";\".join([p.strip().replace(\" \", \"\") for p in s.split(\";\") if p.strip()])\n",
    "            return norm_rank(gt).lower() == norm_rank(pred).lower()\n",
    "\n",
    "        if dtype in {'SITE_WITH_FLOAT', 'SITE_WITH_KEYED_METRICS'}:\n",
    "            # structured: compare normalized strings\n",
    "            return gt_l == pred_l\n",
    "\n",
    "        # fallback\n",
    "        return gt_l == pred_l\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def calculate_calibration(df):\n",
    "    \"\"\"\n",
    "    Brier Score: mean squared error between predicted confidence and correctness.\n",
    "    Lower is better; 0.0 is perfect.\n",
    "    \"\"\"\n",
    "    valid_df = df[df['parsed_confidence'].notna()].copy()\n",
    "    if len(valid_df) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    y = valid_df['is_correct'].astype(int).to_numpy()\n",
    "    p = valid_df['parsed_confidence'].astype(float).to_numpy()\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "\n",
    "    brier = np.mean((p - y) ** 2)\n",
    "    return float(brier)\n",
    "\n",
    "\n",
    "def calculate_auc(df):\n",
    "    \"\"\"\n",
    "    ROC AUC of confidence as a score for correctness (binary label).\n",
    "    Rank-based formula (Mannâ€“Whitney), handles ties, no sklearn dependency.\n",
    "    \"\"\"\n",
    "    valid_df = df[df['parsed_confidence'].notna()].copy()\n",
    "\n",
    "    # AUC undefined if only one class\n",
    "    if len(valid_df) < 2 or valid_df['is_correct'].nunique() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    y_true = valid_df['is_correct'].astype(int).to_numpy()\n",
    "    y_score = valid_df['parsed_confidence'].astype(float).to_numpy()\n",
    "\n",
    "    n_pos = int(y_true.sum())\n",
    "    n_neg = int(len(y_true) - n_pos)\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return np.nan\n",
    "\n",
    "    ranks = pd.Series(y_score).rank(method=\"average\").to_numpy()\n",
    "    sum_ranks_pos = ranks[y_true == 1].sum()\n",
    "\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "# ==========================================\n",
    "# 4. FINAL REPORT GENERATION (THE 3 CSVs)\n",
    "# ==========================================\n",
    "\n",
    "def generate_final_report():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š GENERATING FINAL REPORTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    all_data = []\n",
    "    model_summaries = []\n",
    "\n",
    "    for model_name, _ in MODELS:\n",
    "        result_file = os.path.join(OUTPUT_DIR, model_name, \"results.jsonl\")\n",
    "        if not os.path.exists(result_file):\n",
    "            print(f\"No results found for {model_name}\")\n",
    "            continue\n",
    "\n",
    "        model_results = pd.read_json(result_file, lines=True)\n",
    "        model_results['question_id'] = model_results['question_id'].astype(int)\n",
    "\n",
    "        # Merge using question_id\n",
    "        df = pd.merge(QUESTIONS_DF, model_results, on='question_id', how='inner')\n",
    "        df['Model'] = model_name\n",
    "\n",
    "        # Apply Scoring\n",
    "        parsed = df['llm_response'].apply(validate_and_parse_response)\n",
    "        df['format_valid'] = parsed.apply(lambda x: x[0])\n",
    "        df['parsed_answer'] = parsed.apply(lambda x: x[1])\n",
    "        df['parsed_confidence'] = parsed.apply(lambda x: x[2])\n",
    "        df['is_correct'] = df.apply(check_exactness, axis=1)\n",
    "\n",
    "        all_data.append(df)\n",
    "\n",
    "        # Stats\n",
    "        acc = float(df['is_correct'].mean())\n",
    "        fmt = float(df['format_valid'].mean())\n",
    "        brier = calculate_calibration(df)\n",
    "        auc = calculate_auc(df)\n",
    "\n",
    "        model_summaries.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Format_Adherence\": fmt,\n",
    "            \"Brier (Calibration)\": brier,\n",
    "            \"AUC\": auc\n",
    "        })\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data found to report on.\")\n",
    "        return\n",
    "\n",
    "    # CSV 1: DETAILED BY QUESTION\n",
    "    big_df = pd.concat(all_data, ignore_index=True)\n",
    "    cols = [\n",
    "        'Model', 'question_id', 'Question', 'Type', 'Category',\n",
    "        'Answer', 'parsed_answer', 'parsed_confidence',\n",
    "        'is_correct', 'format_valid', 'llm_response'\n",
    "    ]\n",
    "    safe_cols = [c for c in cols if c in big_df.columns]\n",
    "    big_df[safe_cols].to_csv(\"benchmark_1_detailed.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_1_detailed.csv\")\n",
    "\n",
    "    # CSV 2: SUMMARY BY MODEL\n",
    "    summary_df = pd.DataFrame(model_summaries)\n",
    "    summary_df.to_csv(\"benchmark_2_by_model.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_2_by_model.csv\")\n",
    "\n",
    "        # CSV 3: SUMMARY BY MODEL x TYPE (and Category)\n",
    "    by_model_by_type = (\n",
    "        big_df\n",
    "        .groupby(['Model', 'Type', 'Category'], as_index=False)\n",
    "        .agg(\n",
    "            N=('question_id', 'count'),\n",
    "            Accuracy=('is_correct', 'mean'),\n",
    "            Format_Adherence=('format_valid', 'mean'),\n",
    "            Mean_Confidence=('parsed_confidence', 'mean'),\n",
    "        )\n",
    "    )\n",
    "    by_model_by_type.to_csv(\"benchmark_3_by_model_by_type.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_3_by_model_by_type.csv\")\n",
    "\n",
    "    # CSV 4: GLOBAL STATS BY TYPE (and Category) across all models\n",
    "    global_stats = (\n",
    "        big_df\n",
    "        .groupby(['Type', 'Category'], as_index=False)\n",
    "        .agg(\n",
    "            N=('question_id', 'count'),\n",
    "            Accuracy=('is_correct', 'mean'),\n",
    "            Format_Adherence=('format_valid', 'mean'),\n",
    "            Mean_Confidence=('parsed_confidence', 'mean'),\n",
    "        )\n",
    "    )\n",
    "    global_stats.to_csv(\"benchmark_4_global_stats.csv\", index=False)\n",
    "    print(\"âœ… Saved: benchmark_4_global_stats.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION FLOW\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Environment: 2x A100 | Context: {CTX_SIZE} | Max Tokens: {MAX_TOKENS} ---\")\n",
    "\n",
    "    for friendly_name, model_path in MODELS:\n",
    "        print(f\"\\n{'=' * 50}\\nSTARTING: {friendly_name}\\n{'=' * 50}\")\n",
    "\n",
    "        server_proc = start_server(model_path, model_name=friendly_name)\n",
    "        if server_proc is None:\n",
    "            print(f\"Skipping {friendly_name} due to server failure.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            run_benchmark(friendly_name)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user.\")\n",
    "            if server_proc:\n",
    "                server_proc.kill()\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error: {e}\")\n",
    "        finally:\n",
    "            print(f\"Shutting down {friendly_name}...\")\n",
    "            if server_proc:\n",
    "                server_proc.terminate()\n",
    "                try:\n",
    "                    server_proc.wait(timeout=5)\n",
    "                except Exception:\n",
    "                    server_proc.kill()\n",
    "            time.sleep(3)\n",
    "\n",
    "    generate_final_report()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
